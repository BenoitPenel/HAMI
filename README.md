# HAMI : "Human Assited Molecular Identification" Framework
**A bioninformatics pipeline for processing Frogs output data, in accordance with the HAMI framework**

- **Please cite:** [![doi](https://img.shields.io/static/v1?label=doi&message=DOI)](DOI)
<br />

**Towards large-scale monitoring of biodiversity: a Human-Assisted Molecular Identification (HAMI) framework using metabarcoding while accounting for abundances and systemic errors**
<br />
*Benoit Penel, Laure Benoit, Axel Bourdonn√©, Laurent Soldati, Gael Kergoat, Julien Haran and Christine Meynard* <br />
*Method in Ecology and Evolution, XXXXX, XXXXX 2024*<br />
<br />


## Description  

Snakemake is a workflow management system written in Python. It facilitates the creation and execution of complex data analysis pipelines, particularly in bioinformatics and computational biology. It is associated to a config.yaml file as well as a environment.yaml file. Please check snakemake Tutorial for futher explications: [Snakemake tutorials](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html)

Here HAMI_PIPELINE.smk is a snakemake workflow associated to HAMI framework. It is associateds to config.yaml file and HAMI_environment.yaml file.
Please check all these files to understand how it works.


The role of this snakemake pipeline is to process the sequencing data at the output of the FROGS pipeline as part of the HAMI framework.
There are five successive rules:

- All : It represents the final outputs that need to be generated by the workflow. It triggers the execution of the entire workflow.

- Clean_and_Chimeres : R process for removing chimeric sequences using dada2 packages (Callahan et al., 2016 - Nature Methods)

- Separate_BARCODIN_METABARCODING_data : Python process to discriminate between metabarcoding and barcoding samples

- Filter_METABARCODING_DATA: R process to filter data and eliminate noise and contamination (3 filter steps)

- Pseudogene_Filter_and_reduce_redundancy : Python process to delete pseudogene sequence and reduce data redundancy associates to intraspecific diversty

  <br />

## Installation
### Requirements

Before installation, the following packages should be available on your system must be installed on your system:

* Python >=3.0
* Miniconda3. Please choose the installer corresponding to your OS: [Miniconda dowloads](https://docs.conda.io/en/latest/miniconda.html)
* GIT


Below are debian commands to rapidly install them:
```
sudo apt-get install git
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
# when installation ask if you want to run conda init, answer yes
# after installation ends, reload bash so that conda belongs to your PATH
bash 
```

### Installation

Download HAMI:
``` bash

```

## Setup your environment

A Snakemake environment file is a file that specifies the software dependencies required for running a Snakemake workflow. It typically contains a list of software packages along with their versions, which are necessary for executing the various steps or rules defined in the workflow. The environment file ensures reproducibility by providing a consistent software environment for running the workflow across different computing environments. Please used this environement file to create a conda environement adapted to run HAMI framework (HAMI_PIPELINE.smk).
<br />
Do it as follow : 
conda env create -f HAMI_environment.yaml  --name HAMI_environment
<br />
Then activate it:
conda activate HAMI_environment
<br />

## Setup your config file

When the conda environment has been setup, it time to adapt the config file according to your need for your project.

A Snakemake configuration file (config.yaml) is a file written in Python that contains configuration settings for a Snakemake workflow. The configuration file allows users to customize various aspects of the workflow, such as file paths and other workflow-specific settings.

Here is the list of specific setting which are necessary for running HAMI pipeline:

- Name of projet
- Name of the DNA fragment 
- Number of treads used to run some internal process
- Path to your directory project, as well as data directory and script directory

- Target taxonomic group
- ADN length
- Reading frame of the DNA (Check it before running the pipeline)
- List of the codon stop associated to your type of DNA and organisms
- Arbitrary threshold to discriminated intra-interspecific genetic distance  (usually 97% for animals according to Hebert et al 2003 - Proceedings of the Royal Society of London)


Note that for running HAMI pipeline in contexte of HAMI framework, your samples need to be discrimated based on their name. Please use alphabectic prefix and number for it e.g : CMEY0001. Also cuplicate of samples is necessary. Discrimination between duplicate will be done using suffix : e.g CMEY0001A / CMEY0001B. Information allowing discrimination between samples will thus be implemented in the config file!

- Samples prefix for metabarcoding samples, barcoding (if implemented), and negative control
- Number of digits following the prefix
- Samples suffix for discriminating duplicate


## Test and run your pipeline

It is strongly recommended to test if your configuration is valid and matches the analyses you intended. To do so, launch a dry run of the pipeline using the command:

``` bash
snakemake --snakefile [snakefile].smk -np
```
If no error is throwed, you can lauch the analysis :

``` bash
snakemake --snakefile [snakefile].smk --cores [#cores]
```

## Contact
*Benoit Penel*
PhD student from CBGP laboratory, Montpellier, France.

## Licence
HAMI is availale under the XXXXX license
