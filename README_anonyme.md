# HAMI : "Human Assited Molecular Identification" Framework
**A bioninformatics pipeline for processing Frogs output data, in accordance with the HAMI framework**

- **Please cite:** [![doi](https://img.shields.io/static/v1?label=doi&message=DOI)](DOI)
<br />

**Towards large-scale monitoring of biodiversity: a Human-Assisted Molecular Identification (HAMI) framework using metabarcoding while accounting for abundances and systemic errors**
<br />
*AUTHORS* <br />
*Method in Ecology and Evolution, XXXXX, XXXXX 2024*<br />
<br />

## Prerequisites

The role of HAMI snakemake pipeline is to process the output of the FROGS pipeline as part of the HAMI framework. To run HAMI pipeline, abundance data and multi-affiliations data from FROGS are thus necessary. 
<br />
<br />
Note that for running HAMI pipeline in contexte of HAMI framework, it is also necessary that your samples can be discrimated based on their name. Please use alphabectic prefix  for it e.g : 
Metabarcoding samples = META0001 while Barcoding sample = BAR0001 and control samples = NC(P/p | I/i | E/e)00001.
<br />
<br />
Also, duplicate of metabarcoding samples and control samples are necessary. Discrimination between duplicate will be done using suffix : e.g META0001-A / META0001-B. 
Information allowing discrimination between samples will thus be implemented in the config file (see below).

## Description  

### Tree structure :

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <ul>
        <li><strong>HAMI</strong>
            <ul>
                <li><strong>DATA</strong>  
                    <ul>
                        <li>test_data 
                            <ul>
                                <li>[testCOI_abundance_raw.tsv].tabular</li>
                                <li>[testCOI_multi-affiliations.tsv].tabular</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>HAMI_PIPELINE</strong> 
                    <ul>
                        <li>config.yaml</li>  
                        <li>HAMI_environment.yaml</li>  
                        <li>HAMI_PIPELINE.smk</li>   
                    </ul>
                </li>
                <li><strong>SCRIPTS</strong> 
                    <ul>
                        <li>clean_frogs.R</li>  
                        <li>filter_frogs.R</li>  
                        <li>pseudogene_and_redudancy.py</li>  
                        <li>Separate_data.py</li>  
                        <li>BARCODE_MAKER.py</li> 
                        <li>BARCODE_pseudogene_filter.py</li> 
                    </ul>
                </li>
              <li>ReadME.md</li>
            </ul>
        </li>
    </ul>
</body>
</html>

5 directories, 12 files

### Pipeline description :                   

Snakemake is a workflow management system written in Python. It facilitates the creation and execution of complex data analysis pipelines, particularly in bioinformatics and computational biology. It is associated to a config.yaml file as well as a environment.yaml file. Please check snakemake Tutorial for futher explications: [Snakemake tutorials](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html)

Here HAMI_PIPELINE.smk is a snakemake workflow associated to HAMI framework. It is associated to a config file (config.yaml) and an environement file (HAMI_environment.yaml).
Please check these files to understand how it works.


The role of this snakemake pipeline is to process the sequencing data at the output of the FROGS pipeline as part of the HAMI framework.
To do this, there are five successive rules:

- All : It represents the final outputs that need to be generated by the workflow. It triggers the execution of the entire workflow.

- Clean_and_Chimeres : R process to clean taxonomic affiliation names associated to each FROGS OTU and removing chimeric sequences using dada2 packages (Callahan et al., 2016 - Nature Methods)

- Separate_BARCODING_METABARCODING_data : Python process to discriminate between metabarcoding and barcoding samples

- Filter_METABARCODING_DATA: R process to filter data and eliminate noise and contamination (3 filter steps described in Galan et al., 2016 - MSystems and automated in Chapuis et al., 2023 - Molecular Ecology Resources)

- Pseudogene_Filter_and_reduce_redundancy : Python process to delete pseudogene sequence and reduce data redundancy associated with intraspecific diversty

Each rule call a specific R or Python script saved in SCRIPTS directory. Two additional scripts are available. They allow to delete pseudogene data in barcoding data and to produce full length COI barcode according to the optional step of HAMI framework, which are no implemented in the HAMI pipeline.

## Outputs :

### Tree structures :
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <ul>
        <li>test_data
            <ul>
                <li><strong>BARCODE</strong>
                    <ul>
                        <li> <strong>abundance_raw_data_BARCODING.tsv </strong></li>
                    </ul>
                </li>
                <li><strong>Chimeras</strong>
                    <ul>
                        <li><em> 2 files</em></li>
                    </ul>
                </li>
                <li><strong>METABARCODING</strong>
                    <ul>
                        <li>final_files</li>
                             <ul>
                                <li><strong>final_abundance_file.tsv</strong></li>
                                <li><strong>cluster_seq_keep.txt</strong></li>
                                <li><strong>cluster_merged.txt</strong></li>
                                <li><strong>summary.txt</strong></li>
                             </ul>
                        <li>intermediary_step</li>
                            <ul>   
                                 <li>pseudogene</li>
                                        <ul>
                                            <li><em> 2 files</em></li>
                                        </ul>
                                 <li><em>4 files</em></li>
                            </ul>
                        <li>Rproducts</li>
                             <ul>
                                 <li><em>13 files</em></li>
                             </ul>
                        <li><em>4 files</em></li>
                    </ul>
                </li>
                <li>[testCOI_abundance_raw.tsv].tabular</li>
                <li>[testCOI_multi-affiliations.tsv].tabular</li>
            </ul>
        </li>
    </ul>
</body>
</html>

8 directories, 32 files


### Final outputs :

According to metabarcoding data, 3 final outputs are produced and saved in DATA/our_data/METABARCODING/final_files repository.

- final_abundance_file.tsv : It is an occurrence table broadly similar to that produced by FROGS, but in which names have been revised and clusters with similar taxonomic affiliation and percent identity have been merged. Each taxonomic affiliation in the occurrence table is associated with the number of clusters that have been merged to produce that specific OTU and the range of variance in percentage of identity corresponding to the merged clusters. The "multi" variable is used to discriminate OTUs with taxonomic affiliations associated with the "Multi-affiliation" term or identity percentages associated with the "multi-identity" term. See FROGS documentation for the meaning of these terms.  

- cluster_merged.txt : It is a file which records the name of each cluster associated with the merged OTUs in the occurence_file.tsv file.

- cluster_seq_keep.txt : Itis a fasta file in which all the DNA sequences that have passed all the filters applied during the HAMI pipeline are recorded and labelled with their taxonomic affiliation and cluster number.

- summary.txt : It is a summary file which gave general information about how much taxonomic affiliation, cluster and number of reads were kept in final_abundance_file  and how much have been deleted during pseudogene curation step.

According to barcoding data, 1 final outputs is produced and saved in DATA/our_data/BARCODE repository.

- abundance_raw_data_BARCODING.tsv : It is an occurrence table similar to that produced by FROGS, but in which names have been revised and chimera sequence have been deleted. Future implementations of this pipeline could be envisaged to process pseudogenes in the same way as metabarcoding data. 




  <br />

## Installation
### Requirements

Before installation, the following packages should be available on your system must be installed on your system:

* Python >=3.0
* Miniconda3. Please choose the installer corresponding to your OS: [Miniconda dowloads](https://docs.conda.io/en/latest/miniconda.html)
* GIT


Below are debian commands to rapidly install them:
```
sudo apt-get install git
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
```
When installation asks if you want to run conda init, answer yes.
  <br />
After installation ends, reload bash
```
bash 
```

### Installation
First of all, create a directory that corresponds to your project directory.

Download HAMI:
``` bash
git clone --recursive https://github.com/AUTHOR_REPOSITORY/HAMI.git
cd HAMI
```
When the git clone is finished, you should find a tree structure similar to the one shown at the beginning of this file.
To ensure the functionality of the HAMIE pipeline, no changes should be made to the tree structure.
Simply import the data that you wish to process in relation to your project into the /HAMI/DATA/ directory.
As a reminder, this pipeline processes the output data from FROGS pipeline. 


## Setup your environment

A Snakemake environment file is a file that specifies the software dependencies required for running a Snakemake workflow. It typically contains a list of software packages along with their versions, which are necessary for executing the various steps or rules defined in the workflow. The environment file ensures reproducibility by providing a consistent software environment for running the workflow across different computing environments. Please used this environement file to create a conda environement adapted to run HAMI framework (HAMI_PIPELINE.smk).
<br />
<br />
Do it as follows: 
``` bash
conda env create -f HAMI_PIPELINE/HAMI_environment.yaml  --name HAMI_environment
``` 
<br />

## Activate your environment

Before running HAMI pipeline, the previously created environement need to be activate as follows:

``` bash
conda activate HAMI_environment
```
To deactivate your environment use the following line code : 

``` bash
conda deactivate 
```
Don't forget to reactivate your conda environment each time you re-use the pipeline. 

<br />

## Setup your config file

When the conda environment has been setup, it is time to adapt the config file according to your need for your project.

A Snakemake configuration file (config.yaml) is a file written in Python that contains configuration settings for a Snakemake workflow. The configuration file allows users to customize various aspects of the workflow, such as file paths and other workflow-specific settings.

Here is the list of specific setting which are necessary for running HAMI pipeline:

- Name of projet
- Name of the DNA fragment
- Number of treads used to run some internal process
- Path to your directory project, as well as data directory and script directory
- Name of the two first inputs used in HAMI pipeline (or FROGS outputs : abundance and multi-affiliation files )

- Target taxonomic group
- ADN length
- Reading frame of the DNA (Check it before running the pipeline)
- List of the codon stop associated to your type of DNA and organisms
- Arbitrary threshold to discriminated intra-interspecific genetic distance  (usually 97% for animals according to Hebert et al 2003 - Proceedings of the Royal Society of London)

- Samples prefix for metabarcoding samples, barcoding (if implemented), and negative control
- Samples suffix for discriminating duplicate


## Test and run your pipeline

To launch the pipeline from a terminal, go to the HAMI directory.

It is strongly recommended to test if your configuration is valid and matches the analyses you intended. To do so, launch a dry run of the pipeline using the command:

``` bash
snakemake --snakefile HAMI_PIPELINE/HAMI_PIPELINE.smk -np
```
If no error is throwed, you can lauch the pipeline :

``` bash
snakemake --snakefile HAMI_PIPELINE/HAMI_PIPELINE.smk --cores [#cores] all
```
(note that the first launch is quite slow due to R packages installation from BioConductor)

## Contact
*CORRESPONDING AUTHOR*
CONTACTS

## Licence
HAMI is availale under the Creative Commons Zero v1.0 Universal license
